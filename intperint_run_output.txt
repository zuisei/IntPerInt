[INFO] 既に存在: /Users/0xt4/Library/Application Support/IntPerInt/Models/tiny-mistral-Q4_K_M.gguf
[STEP] モデルサイズ:
-rw-r--r--  1 0xt4  staff   127M  8 18 09:21 /Users/0xt4/Library/Application Support/IntPerInt/Models/tiny-mistral-Q4_K_M.gguf

[INFO] /tmp/llama.cpp exists, skipping clone
[STEP] cmake build...
CMAKE_BUILD_TYPE=Release
-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- GGML_SYSTEM_ARCH: ARM
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:79 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:372 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- ARM -mcpu not found, -mcpu=native will be used
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+nosme 
-- BLAS found, Libraries: /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- Including METAL backend
-- ggml version: 0.0.1
-- ggml commit:  19f4dec
-- Configuring done (0.4s)
-- Generating done (0.2s)
-- Build files have been written to: /tmp/llama.cpp/build
[  0%] Built target build_info
[  1%] Built target sha1
[  1%] Built target sha256
[  2%] Built target xxhash
[  3%] Built target llama-gemma3-cli
[  3%] Built target llama-qwen2vl-cli
[  5%] Built target llama-minicpmv-cli
[  5%] Built target llama-llava-cli
[  9%] Built target ggml-base
[ 10%] Built target ggml-metal
[ 11%] Built target ggml-blas
[ 17%] Built target ggml-cpu
[ 18%] Built target ggml
[ 20%] Built target llama-gguf
[ 20%] Built target llama-gguf-hash
[ 31%] Built target llama
[ 32%] Built target test-c
[ 32%] Built target llama-simple-chat
[ 33%] Built target llama-simple
[ 35%] Built target mtmd
[ 40%] Built target common
[ 41%] Built target test-tokenizer-0
[ 43%] Built target test-grammar-parser
[ 45%] Built target test-sampling
[ 46%] Built target test-log
[ 47%] Built target test-tokenizer-1-spm
[ 48%] Built target test-gbnf-validator
[ 49%] Built target test-quantize-stats
[ 50%] Built target test-tokenizer-1-bpe
[ 51%] Built target test-chat-parser
[ 51%] Built target test-regex-partial
[ 52%] Built target test-llama-grammar
[ 53%] Built target test-grammar-integration
[ 54%] Built target test-json-schema-to-grammar
[ 55%] Built target test-json-partial
[ 56%] Built target test-chat
[ 57%] Built target test-chat-template
[ 58%] Built target test-thread-safety
[ 59%] Built target test-model-load-cancel
[ 60%] Built target test-opt
[ 62%] Built target test-mtmd-c-api
[ 63%] Built target test-arg-parser
[ 65%] Built target test-backend-ops
[ 68%] Built target test-quantize-fns
[ 68%] Built target test-rope
[ 68%] Built target test-gguf
[ 71%] Built target test-autorelease
[ 71%] Built target test-quantize-perf
[ 71%] Built target llama-batched
[ 71%] Built target llama-embedding
[ 72%] Built target llama-gritlm
[ 73%] Built target llama-eval-callback
[ 74%] Built target test-barrier
[ 75%] Built target llama-lookahead
[ 76%] Built target llama-lookup
[ 76%] Built target llama-lookup-merge
[ 77%] Built target llama-lookup-create
[ 78%] Built target llama-lookup-stats
[ 78%] Built target llama-parallel
[ 79%] Built target llama-passkey
[ 80%] Built target llama-retrieval
[ 81%] Built target llama-save-load-state
[ 82%] Built target llama-finetune
[ 82%] Built target llama-gen-docs
[ 83%] Built target llama-speculative
[ 84%] Built target llama-diffusion-cli
[ 85%] Built target llama-convert-llama2c-to-ggml
[ 86%] Built target llama-batched-bench
[ 86%] Built target llama-vdot
[ 87%] Built target llama-q8dot
[ 87%] Built target llama-imatrix
[ 87%] Built target llama-cli
[ 88%] Built target llama-speculative-simple
[ 89%] Built target llama-gguf-split
[ 90%] Built target llama-bench
[ 92%] Built target llama-quantize
[ 92%] Built target llama-perplexity
[ 94%] Built target llama-server
[ 95%] Built target llama-tokenize
[ 98%] Built target llama-mtmd-cli
[ 98%] Built target llama-tts
[ 98%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[100%] Built target llama-run
[OK] ビルド完了. Build dir: /tmp/llama.cpp/build
total 528
drwxr-xr-x  23 0xt4  wheel   736B  8 18 09:28 .
drwxr-xr-x@ 56 0xt4  wheel   1.8K  8 17 23:27 ..
drwxr-xr-x@  5 0xt4  wheel   160B  8 17 23:27 autogenerated
drwxr-xr-x@ 84 0xt4  wheel   2.6K  8 17 23:28 bin
-rw-r--r--@  1 0xt4  wheel   4.7K  8 18 09:28 cmake_install.cmake
-rw-r--r--@  1 0xt4  wheel    35K  8 18 09:21 CMakeCache.txt
drwxr-xr-x@ 42 0xt4  wheel   1.3K  8 18 09:28 CMakeFiles
drwxr-xr-x@  7 0xt4  wheel   224B  8 18 09:28 common
-rw-r--r--@  1 0xt4  wheel   131K  8 18 09:28 compile_commands.json
-rw-r--r--@  1 0xt4  wheel   366B  8 18 09:28 CTestTestfile.cmake
-rw-r--r--@  1 0xt4  wheel   2.5K  8 17 23:27 DartConfiguration.tcl
drwxr-xr-x@ 26 0xt4  wheel   832B  8 18 09:28 examples
drwxr-xr-x@  8 0xt4  wheel   256B  8 18 09:28 ggml
-rw-r--r--   1 0xt4  wheel    56B  8 18 09:22 intperint_run_output.txt
-rw-r--r--@  1 0xt4  wheel   1.7K  8 17 23:27 llama-config.cmake
-rw-r--r--@  1 0xt4  wheel   2.7K  8 17 23:27 llama-version.cmake
-rw-r--r--@  1 0xt4  wheel   243B  8 17 23:27 llama.pc
-rw-r--r--@  1 0xt4  wheel    63K  8 18 09:28 Makefile
drwxr-xr-x@  7 0xt4  wheel   224B  8 18 09:28 pocs
drwxr-xr-x@  5 0xt4  wheel   160B  8 18 09:28 src
drwxr-xr-x@  3 0xt4  wheel    96B  8 17 23:27 Testing
drwxr-xr-x@  6 0xt4  wheel   192B  8 18 09:28 tests
drwxr-xr-x@ 20 0xt4  wheel   640B  8 18 09:28 tools

[INFO] 使用実行バイナリ: /tmp/llama.cpp/build/bin/llama-cli
[TRY] /tmp/llama.cpp/build/bin/llama-cli -m "/Users/0xt4/Library/Application Support/IntPerInt/Models/tiny-mistral-Q4_K_M.gguf" -p "Hello" -n 8
build: 1 (19f4dec) with Apple clang version 17.0.0 (clang-1700.0.13.5) for arm64-apple-darwin24.6.0
[INFO] コマンド実行OK (rc=124): /tmp/llama.cpp/build/bin/llama-cli -m "/Users/0xt4/Library/Application Support/IntPerInt/Models/tiny-mistral-Q4_K_M.gguf" -p "Hello" -n 8
[STEP] 成功パターンで30秒間ストリーミング読み取りを試します...
build: 1 (19f4dec) with Apple clang version 17.0.0 (clang-1700.0.13.5) for arm64-apple-darwin24.6.0
